PRE-MIDTERM (Jan 8 - Feb 15)
p3
Task 1
Task 2
Task 3

Pthreads
Week 5 : 2.4, 2.6, 4.1 - 4.3
Pthreads - Critical Sections
Week 6 : 4.4 - 4.7

POST-MIDTERM (Feb 16 - Mar20)
p6
(see Open MP)
Task 1 using OPEN mp to run LU decompisition w/ nthreads
Task 2 break LU decomposition into blocks
Task 3

Pthreads - Barriers Read/Write Locks

Week 7 : 4.8 - 4.9

Pthreads - False Sharing, Thread Safety
Week 8 : 4.10 - 4.12


Open MP
Week 8 : 5.1 - 5.4
p230 (OpenMP vs Pthreads)
Although OpenMP and Pthreads are both APIs for shared-memory programming, they have many fundamental differences. Pthreads requires that the programmer explicitly specify the behavior of each thread. OpenMP, on the other hand, sometimes allows the programmer to simply state that
a block of code should be executed in parallel, and the precise determination of the tasks and which thread should execute them is left to the compiler and the run-time system.
p231 (possible final topics from Ch5)
Learn how to exploit OpenMP's ability to parallelize many serial for loops with only small changes to source code (PR6, task 1)
Standard problems w/shared-memory programming
Effect of cache memories on shared-memory programming
Problems that can be encountered when serial code/library is used in a shared-memory program
p235 # pragma omp parallel
What actually happens when the program gets to the parallel directive?
Prior to the parallel directive, the program is using a single thread, the process started when the program started execution. When the program reaches the parallel directive, the original thread continues executing adn thead_count - 1 additonal threads are started.
In OpenMP parlance, the collection of threads executing the parallel block (new & original) are called a team, the orignal thread the master, the rest slaves. Each thread in the team executes the block following the directive.

OpenMP Directive (e.g. parallel) specifies that the structed block of code that follows should be executed by multiple threads
     Structed block is a C statement, one point of entry, one point of exit
OpenMP Clause (e.g. num_threads(count)) is just some text that modifies the directive

Recall definitions of Race Condition and Critical Section
#pragma omp critical

A reduction oeprator is a binary operation and a reduction is a computation that repeatedly applies the same reduction operator to a sequence of operands in order to get a singal result.  All of the intermmediate results of the operation should e stored in the same variable: the reduction variable.
e.g.
          global_result = 0.0
#       pragma omp parallel num_threads(thread_count) \
               reduction(+: global_result)
          global_result += Local_trap(double a, double b, int n);

equivalent to
        global_result = 0.0;
#      pragma omp parallel num_threads(thread_count)
        {
               double my_result = 0.0;
               my_result += Local_trap(double a, double b, int n);
#             pragma omp critical
               global_result += my_result;
         }

On final point to note is that the threads' private vaiables are initialize to 0.  For a reduction clause they're initalized to the identity value for the operator.

OpenMP Directive   Parallel For
     h = (b-a)/n;
     approx = (f(a) + f(b))/2.0
#   pragma omp parallel for num_threads(thread_count) \
          reduction(+: approx)
     for (i = 1; i <= n-1; i++)
          approx += f(a + i*h);
     approx = h * approx;

The parallel for directive is therefore very different from the parallel directive, because in a block that is preceded by a parallel directive, in general, the work must be divided amont the threads by the threads themselves (e.g. local_a, local_b, etc.)
In a for loop that has been parallelized with a parallel for directive, the default partitioning, that is, of the iterations among the threads is up the the system. (usally block partitioning)

Week 9 : 5.5-  5.8
Readings
Slides
Cholesky LU Factorization
If [A] is symmetric and positive definite, it is convenient ot use Cholesky Decomposition
[A] = [L][L]^T = [U]^T[U]

(visualizations)

Block-based version of Cholesky
For computation efficiency on processors with cache, the Cholesky decomposition algorithm needs some adjustment
The basic idea:
-Decompose an N x N matrix inot n_b x n_b blocks
-We will only consider the case were N is evenly divided by n_b.
     -The general case isn't much more difficult
-Rewrite the decomposition in block form
-Take advantage of the special structure of specific blocks
-Define an UPDATE to a matrix (this does more work, so payoff better be worth it!)

(visualiztions)
